{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d5e09b-13bb-47f8-b728-524b7842cb11",
   "metadata": {},
   "source": [
    "# T5 Client Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d15d781-9f77-4b4d-b42b-0864991c8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import T5Tokenizer, T5TokenizerFast  # transformers-4.10.0-py3\n",
    "import tritonclient.http as httpclient\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import np_to_triton_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbfd65a-4348-4cbc-93c9-0a6af357c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"172.25.4.4\"\n",
    "MODEL_T5_HUGGINGFACE = \"t5-3b\"  # we need this to import tokenizers\n",
    "MODEl_T5_FASTERTRANSFORMER = \"fastertransformer\"  # name of the model at the TRITON side\n",
    "VERBOSE = False\n",
    "\n",
    "torch.set_printoptions(precision=6)\n",
    "\n",
    "GRPC = 1\n",
    "\n",
    "if GRPC:\n",
    "    URL = \"{}:8001\".format(URL)\n",
    "    client = grpcclient.InferenceServerClient(url=URL, verbose=VERBOSE)\n",
    "else:\n",
    "    URL = \"{}:8000\".format(URL)\n",
    "    request_parallelism = 10\n",
    "    client = httpclient.InferenceServerClient(\n",
    "        URL, concurrency=request_parallelism, verbose=VERBOSE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24316ffd-9529-4277-85b3-8db23ce00029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_T5_HUGGINGFACE, model_max_length=1024)\n",
    "fast_tokenizer = T5TokenizerFast.from_pretrained(\n",
    "    MODEL_T5_HUGGINGFACE, model_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c094f0-697b-476e-b744-affe079be0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(t5_task_input, grpc=0):\n",
    "    input_token = tokenizer(t5_task_input, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = input_token.input_ids.numpy().astype(np.uint32)\n",
    "\n",
    "    mem_seq_len = torch.sum(input_token.attention_mask, dim=1).numpy().astype(np.uint32)\n",
    "    mem_seq_len = mem_seq_len.reshape([mem_seq_len.shape[0], 1])\n",
    "    max_output_len = np.array([[128]], dtype=np.uint32)\n",
    "    runtime_top_k = (1.0 * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "    if grpc:\n",
    "        tritonclient=grpcclient\n",
    "    else:\n",
    "        tritonclient=httpclient\n",
    "        \n",
    "\n",
    "    inputs = [\n",
    "        tritonclient.InferInput(\n",
    "            \"input_ids\", input_ids.shape, np_to_triton_dtype(input_ids.dtype)\n",
    "        ),\n",
    "        tritonclient.InferInput(\n",
    "            \"sequence_length\",\n",
    "            mem_seq_len.shape,\n",
    "            np_to_triton_dtype(mem_seq_len.dtype),\n",
    "        ),\n",
    "        tritonclient.InferInput(\n",
    "            \"max_output_len\",\n",
    "            max_output_len.shape,\n",
    "            np_to_triton_dtype(mem_seq_len.dtype),\n",
    "        ),\n",
    "        tritonclient.InferInput(\n",
    "            \"runtime_top_k\",\n",
    "            runtime_top_k.shape,\n",
    "            np_to_triton_dtype(runtime_top_k.dtype),\n",
    "        ),\n",
    "    ]\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(mem_seq_len)\n",
    "    inputs[2].set_data_from_numpy(max_output_len)\n",
    "    inputs[3].set_data_from_numpy(runtime_top_k)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94da6309-9853-444b-8220-8d1ee622a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(t5_task_input):\n",
    "    input_token = tokenizer(t5_task_input, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = input_token.input_ids.numpy().astype(np.uint32)\n",
    "\n",
    "    mem_seq_len = torch.sum(input_token.attention_mask, dim=1).numpy().astype(np.uint32)\n",
    "    mem_seq_len = mem_seq_len.reshape([mem_seq_len.shape[0], 1])\n",
    "    max_output_len = np.array([[128]], dtype=np.uint32)\n",
    "    runtime_top_k = (1.0 * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "    return input_ids, mem_seq_len, max_output_len, runtime_top_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e900f05c-9836-4c4e-8ca9-e59ec91c9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(result):\n",
    "    ft_decoding_outputs = result.as_numpy(\"output_ids\")\n",
    "    ft_decoding_seq_lens = result.as_numpy(\"sequence_length\")\n",
    "    # print(type(ft_decoding_outputs), type(ft_decoding_seq_lens))\n",
    "    # print(ft_decoding_outputs, ft_decoding_seq_lens)\n",
    "    tokens = fast_tokenizer.decode(\n",
    "        ft_decoding_outputs[0][0][: ft_decoding_seq_lens[0][0]],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675010c9-4b6e-4217-9844-292fa8646312",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe732c6-11a4-4441-b0ad-b3d4ef43bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbon monoxide\n"
     ]
    }
   ],
   "source": [
    "text = \"question: What does increased oxygen concentrations in the patient’s lungs displace? \\\n",
    "        context: Hyperbaric (high-pressure) medicine uses specialoxygen chambers to increase the partial pressure \\\n",
    "        of O 2 around the patientand, when needed, the medical staff. Carbon monoxide poisoning, \\\n",
    "        gas gangrene,and decompression sickness (the ’bends’) are sometimes treated using thesedevices. \\\n",
    "        Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. \\\n",
    "        Oxygen gas is poisonous to theanaerobic bacteria that cause gas gangrene, so increasing its partial pressurehelps kill them. \\\n",
    "        Decompression sickness occurs in divers who decompress tooquickly after a dive, resulting in bubbles of inert gas, \\\n",
    "        mostly nitrogen andhelium, forming in their blood. Increasing the pressure of O 2 as soon aspossible is part of the treatment.\"\n",
    "inputs = preprocess(text, grpc=GRPC)\n",
    "result = client.infer(MODEl_T5_FASTERTRANSFORMER, inputs)\n",
    "postprocess(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b612ba-7bc4-4c3e-a87c-1240bf57527c",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12c32cb-788c-47c1-8adf-1d16d4f8aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfer learning has emerged as a powerful technique in natural language processing (NLP) in this paper, we introduce a unified framework that converts all text-based language problems into a text-to-text format. we then train a model on the text-to-text format and apply it to the problem of predicting the meaning of a sentence.\n"
     ]
    }
   ],
   "source": [
    "text = \"summarize: Transfer learning, where a model is first pre-trained on a data-rich task \\\n",
    "        before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language \\\n",
    "        processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, \\\n",
    "        methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP \\\n",
    "        by introducing a unified framework that converts all text-based language problems into a text-to-text format\"\n",
    "inputs = preprocess(text, grpc=GRPC)\n",
    "result = client.infer(MODEl_T5_FASTERTRANSFORMER, inputs)\n",
    "postprocess(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb0c99-e816-414d-ac7b-a66ce8d7ea01",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f00982-078c-4024-b2ea-e52c6684fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er schwenkte den Angelstab zurück und stieß die Angel.\n"
     ]
    }
   ],
   "source": [
    "text = \"Translate English to German: He swung back the fishing pole and cast the line.\"\n",
    "inputs = preprocess(text, grpc=GRPC)\n",
    "result = client.infer(MODEl_T5_FASTERTRANSFORMER, inputs)\n",
    "postprocess(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b149c74d-4307-4ecf-a755-aacf2ca5b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, mem_seq_len, max_output_len, runtime_top_k = generate_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ff481c-cd27-48fd-8cb8-436722887b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[30355,    15,  1566,    12,  2968,    10,   216,     3,     7,\n",
       "           210,   425,   223,     8,  5095, 11148,    11,  4061,     8,\n",
       "           689,     5,     1]], dtype=uint32),\n",
       " array([[21]], dtype=uint32),\n",
       " array([[128]], dtype=uint32),\n",
       " array([[1]], dtype=uint32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, mem_seq_len, max_output_len, runtime_top_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27df7584-630b-4f1a-bdcf-a13c8bfc8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 21), (1, 1), (1, 1), (1, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, mem_seq_len.shape, max_output_len.shape, runtime_top_k .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9c70a-3591-4a0c-9ac9-215ac4c2ffca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
